{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7d0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd\n",
    "from LoadData import load_rating_data, spilt_rating_dat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ProbabilisticMatrixFactorization import PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad0e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, tqdm\n",
    "from torch.utils  import data\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abea8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"data/ml-latest/ratings.csv\")\n",
    "ratings = ratings[['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e043d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"data/ml-100k/u.data\", sep='\\t', header=None)\n",
    "ratings.columns = ['userId', 'movieId', 'rating', 'ts']\n",
    "ratings = ratings[['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4ff5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid2user = dict(zip( ratings.userId.drop_duplicates(), range(ratings.userId.nunique())))\n",
    "iid2item = dict(zip( ratings.movieId.drop_duplicates(), range(ratings.movieId.nunique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6c5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.userId = ratings.userId.map(uid2user)\n",
    "ratings.movieId = ratings.movieId.map(iid2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f6bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = train_test_split(ratings, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0aa396",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_ratings[['userId', 'movieId']].values\n",
    "train_y = train_ratings['rating'].values.astype('float64')\n",
    "test_X = test_ratings[['userId', 'movieId']].values\n",
    "test_y = test_ratings['rating'].values.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "434efe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(train_X).to(device='cuda')\n",
    "train_y = torch.from_numpy(train_y).to(device='cuda')\n",
    "test_X = torch.from_numpy(test_X).to(device='cuda')\n",
    "test_y = torch.from_numpy(test_y).to(device='cuda')\n",
    "train_y_mean = train_y.mean()\n",
    "train_y = train_y - train_y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e09f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data iterater\n",
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10000\n",
    "data_iter = load_array((train_X, train_y), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2a316",
   "metadata": {},
   "source": [
    "# ReRun from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c54dfde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "num_user = len(uid2user)\n",
    "num_item = len(iid2item)\n",
    "num_feat = 20\n",
    "\n",
    "w_Item = torch.normal(0, 0.01, (num_item, num_feat), device='cuda', requires_grad=True)\n",
    "w_User = torch.normal(0, 0.01, (num_user, num_feat), device='cuda', requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "024bf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d347d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizor\n",
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba7e87f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 1.1251444279270384, test loss: 1.1267892413634506\n",
      "epoch: 1, train loss: 1.1251439298767225, test loss: 1.126789220197415\n",
      "epoch: 2, train loss: 1.125143430677381, test loss: 1.1267891985348732\n",
      "epoch: 3, train loss: 1.1251429302985043, test loss: 1.1267891763564644\n",
      "epoch: 4, train loss: 1.125142428740307, test loss: 1.1267891536740333\n",
      "epoch: 5, train loss: 1.1251419259840114, test loss: 1.1267891304667175\n",
      "epoch: 6, train loss: 1.1251414220087785, test loss: 1.1267891067340108\n",
      "epoch: 7, train loss: 1.1251409168036426, test loss: 1.1267890824680709\n",
      "epoch: 8, train loss: 1.1251404103502805, test loss: 1.1267890576585202\n",
      "epoch: 9, train loss: 1.1251399026288114, test loss: 1.1267890322997172\n",
      "epoch: 10, train loss: 1.1251393936257355, test loss: 1.126789006380819\n",
      "epoch: 11, train loss: 1.1251388833303027, test loss: 1.126788979897558\n",
      "epoch: 12, train loss: 1.1251383717206067, test loss: 1.126788952843489\n",
      "epoch: 13, train loss: 1.1251378587917864, test loss: 1.1267889252081105\n",
      "epoch: 14, train loss: 1.12513734451864, test loss: 1.1267888969832487\n",
      "epoch: 15, train loss: 1.125136828881932, test loss: 1.1267888681597353\n",
      "epoch: 16, train loss: 1.1251363118682765, test loss: 1.126788838724644\n",
      "epoch: 17, train loss: 1.1251357934619206, test loss: 1.1267888086781643\n",
      "epoch: 18, train loss: 1.1251352736420084, test loss: 1.126788778008881\n",
      "epoch: 19, train loss: 1.125134752402131, test loss: 1.1267887467086346\n",
      "epoch: 20, train loss: 1.1251342297109923, test loss: 1.1267887147549236\n",
      "epoch: 21, train loss: 1.125133705558279, test loss: 1.126788682160098\n",
      "epoch: 22, train loss: 1.1251331799365296, test loss: 1.1267886489061532\n",
      "epoch: 23, train loss: 1.1251326528201369, test loss: 1.1267886149818873\n",
      "epoch: 24, train loss: 1.125132124184401, test loss: 1.1267885803819593\n",
      "epoch: 25, train loss: 1.1251315940204603, test loss: 1.1267885450908464\n",
      "epoch: 26, train loss: 1.1251310623065969, test loss: 1.1267885091059398\n",
      "epoch: 27, train loss: 1.125130529034518, test loss: 1.126788472419835\n",
      "epoch: 28, train loss: 1.125129994175269, test loss: 1.126788435013143\n",
      "epoch: 29, train loss: 1.125129457707521, test loss: 1.1267883968827777\n",
      "epoch: 30, train loss: 1.1251289196226182, test loss: 1.1267883580175229\n",
      "epoch: 31, train loss: 1.1251283799030107, test loss: 1.126788318412005\n",
      "epoch: 32, train loss: 1.1251278385263797, test loss: 1.1267882780531646\n",
      "epoch: 33, train loss: 1.125127295471598, test loss: 1.1267882369261037\n",
      "epoch: 34, train loss: 1.1251267507145222, test loss: 1.1267881950187302\n",
      "epoch: 35, train loss: 1.1251262042512955, test loss: 1.126788152336925\n",
      "epoch: 36, train loss: 1.125125656052613, test loss: 1.1267881088535794\n",
      "epoch: 37, train loss: 1.1251251061022565, test loss: 1.1267880645651522\n",
      "epoch: 38, train loss: 1.1251245543796051, test loss: 1.1267880194677817\n",
      "epoch: 39, train loss: 1.1251240008642447, test loss: 1.1267879735312114\n",
      "epoch: 40, train loss: 1.1251234455308683, test loss: 1.1267879267595755\n",
      "epoch: 41, train loss: 1.1251228883685234, test loss: 1.1267878791370316\n",
      "epoch: 42, train loss: 1.1251223293573998, test loss: 1.1267878306596775\n",
      "epoch: 43, train loss: 1.1251217684781623, test loss: 1.1267877813048104\n",
      "epoch: 44, train loss: 1.1251212057013045, test loss: 1.126787731073685\n",
      "epoch: 45, train loss: 1.1251206410048225, test loss: 1.1267876799420304\n",
      "epoch: 46, train loss: 1.125120074381152, test loss: 1.1267876279045725\n",
      "epoch: 47, train loss: 1.1251195058059091, test loss: 1.126787574947514\n",
      "epoch: 48, train loss: 1.125118935251932, test loss: 1.1267875210678249\n",
      "epoch: 49, train loss: 1.125118362700025, test loss: 1.126787466238804\n",
      "epoch: 50, train loss: 1.1251177881189347, test loss: 1.1267874104536515\n",
      "epoch: 51, train loss: 1.1251172115019008, test loss: 1.1267873536994657\n",
      "epoch: 52, train loss: 1.1251166328309707, test loss: 1.1267872959718248\n",
      "epoch: 53, train loss: 1.1251160520707792, test loss: 1.1267872372501553\n",
      "epoch: 54, train loss: 1.1251154691964487, test loss: 1.126787177518608\n",
      "epoch: 55, train loss: 1.125114884197994, test loss: 1.126787116769571\n",
      "epoch: 56, train loss: 1.1251142970429255, test loss: 1.1267870549939776\n",
      "epoch: 57, train loss: 1.125113707709104, test loss: 1.126786992168456\n",
      "epoch: 58, train loss: 1.1251131161743257, test loss: 1.126786928272726\n",
      "epoch: 59, train loss: 1.1251125224177694, test loss: 1.1267868633114542\n",
      "epoch: 60, train loss: 1.1251119264220633, test loss: 1.1267867972695522\n",
      "epoch: 61, train loss: 1.1251113281488334, test loss: 1.1267867301258139\n",
      "epoch: 62, train loss: 1.125110727580498, test loss: 1.1267866618677191\n",
      "epoch: 63, train loss: 1.1251101247000228, test loss: 1.1267865924813567\n",
      "epoch: 64, train loss: 1.125109519481044, test loss: 1.1267865219530206\n",
      "epoch: 65, train loss: 1.125108911887422, test loss: 1.126786450258121\n",
      "epoch: 66, train loss: 1.12510830189852, test loss: 1.1267863773992979\n",
      "epoch: 67, train loss: 1.1251076894944125, test loss: 1.1267863033382515\n",
      "epoch: 68, train loss: 1.125107074651762, test loss: 1.1267862280875165\n",
      "epoch: 69, train loss: 1.125106457341403, test loss: 1.1267861516146014\n",
      "epoch: 70, train loss: 1.125105837537453, test loss: 1.1267860739162707\n",
      "epoch: 71, train loss: 1.125105215216958, test loss: 1.1267859949677028\n",
      "epoch: 72, train loss: 1.125104590337493, test loss: 1.1267859147447536\n",
      "epoch: 73, train loss: 1.1251039628817445, test loss: 1.126785833244562\n",
      "epoch: 74, train loss: 1.1251033328362634, test loss: 1.1267857504513983\n",
      "epoch: 75, train loss: 1.1251027001767917, test loss: 1.126785666344152\n",
      "epoch: 76, train loss: 1.1251020648459729, test loss: 1.1267855808976497\n",
      "epoch: 77, train loss: 1.1251014268293116, test loss: 1.1267854941120012\n",
      "epoch: 78, train loss: 1.125100786107601, test loss: 1.1267854059583478\n",
      "epoch: 79, train loss: 1.1251001426464242, test loss: 1.1267853164146235\n",
      "epoch: 80, train loss: 1.1250994964213756, test loss: 1.126785225476362\n",
      "epoch: 81, train loss: 1.1250988474062666, test loss: 1.1267851331219318\n",
      "epoch: 82, train loss: 1.125098195575989, test loss: 1.1267850393344174\n",
      "epoch: 83, train loss: 1.1250975408814343, test loss: 1.1267849440912596\n",
      "epoch: 84, train loss: 1.1250968832966692, test loss: 1.1267848473699626\n",
      "epoch: 85, train loss: 1.125096222796466, test loss: 1.1267847491543013\n",
      "epoch: 86, train loss: 1.125095559362676, test loss: 1.1267846494388225\n",
      "epoch: 87, train loss: 1.1250948929654274, test loss: 1.1267845481984353\n",
      "epoch: 88, train loss: 1.125094223561588, test loss: 1.1267844454119194\n",
      "epoch: 89, train loss: 1.125093551121627, test loss: 1.1267843410520357\n",
      "epoch: 90, train loss: 1.125092875614268, test loss: 1.1267842351049344\n",
      "epoch: 91, train loss: 1.1250921970039518, test loss: 1.126784127548214\n",
      "epoch: 92, train loss: 1.1250915152710224, test loss: 1.1267840183678082\n",
      "epoch: 93, train loss: 1.125090830368112, test loss: 1.1267839075347759\n",
      "epoch: 94, train loss: 1.125090142272716, test loss: 1.126783795029829\n",
      "epoch: 95, train loss: 1.125089450956318, test loss: 1.1267836808396712\n",
      "epoch: 96, train loss: 1.1250887563759127, test loss: 1.1267835649374836\n",
      "epoch: 97, train loss: 1.1250880585065717, test loss: 1.1267834472997773\n",
      "epoch: 98, train loss: 1.1250873573022135, test loss: 1.1267833279140436\n",
      "epoch: 99, train loss: 1.1250866527391707, test loss: 1.1267832067458319\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "stats = []\n",
    "for epoch in range(100):\n",
    "    for X, y in data_iter:\n",
    "        yhat = (w_User[X[:, 0]] * w_Item[X[:, 1]]).sum(axis=1)\n",
    "        l = squared_loss(yhat, y)\n",
    "        l.sum().backward()\n",
    "        with torch.no_grad():\n",
    "            for mat in [w_Item, w_User]:\n",
    "                mat -= lr * mat.grad / batch_size\n",
    "                mat.grad.zero_()\n",
    "    with torch.no_grad():\n",
    "        test_yhat = (w_User[test_X[:, 0]] * w_Item[test_X[:, 1]]).sum(axis=1)\n",
    "        test_l = torch.sqrt(squared_loss(test_yhat, test_y - train_y_mean).mean())\n",
    "        \n",
    "        train_yhat = (w_User[train_X[:, 0]] * w_Item[train_X[:, 1]]).sum(axis=1)\n",
    "        train_l = torch.sqrt(squared_loss(train_yhat, train_y).mean())\n",
    "        \n",
    "        print(f\"epoch: {epoch}, train loss: {train_l}, test loss: {test_l}\")\n",
    "        stats.append([epoch, train_l, test_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad5e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
